# Chat with PDF locally with Ollama demo ðŸš€

If you have any questions or suggestions, please feel free to create an issue in this repository, I will do my best to respond.

Give me a star if you like this repo :) and shoutout to the repo that I fork and please check out the original github-repo: https://github.com/tonykipkemboi/ollama_pdf_rag

## Running the Streamlit application

1. **Clone repo**

2. **Install Dependencies**: Execute to install dependencies
  
      ```bash
      pip install -r requirements.txt
      ```

4. **Pull ollama model**: Get the nomic embed text model
      ```bash
      ollama pull nomic-embed-text
      ```

3. **Launch the App**: Run to start the Streamlit interface on `localhost`

      ```bash
      streamlit run streamlit_app.py
      ``` 
