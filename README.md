# Chat with PDF locally with Ollama demo ðŸš€

If you have any questions or suggestions, please feel free to create an issue in this repository, I will do my best to respond.

Thank you for watching, and I hope you found this resource useful! ðŸ˜Š

## Running the Streamlit application

1. **Clone repo**

2. **Install Dependencies**: Execute to install dependencies
  
      ```bash
      pip install -r requirements.txt
      ```

4. **Pull ollama model**: Get the nomic embed text model
      ```bash
      ollama pull nomic-embed-text
      ```

3. **Launch the App**: Run to start the Streamlit interface on `localhost`

      ```bash
      streamlit run streamlit_app.py
      ``` 
